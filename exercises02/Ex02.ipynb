{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex1\n",
    "\n",
    "a) X, X, X...; left, left, left,...; ,0, 0, 0,...\n",
    "\n",
    "b) X, X, X, X, Y, end;  right, right, right, right, right; +1, +1 , +1, -1, +4 \n",
    "\n",
    "c) $G_0 = 1.875$\n",
    "\n",
    "d) 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.875"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = [1,1,1,-1,4]\n",
    "g = sum([0.5**k*rewards[k] for k in range(len(rewards))])\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex2\n",
    "\n",
    "a) \n",
    "\n",
    "One example might be a robot (or algorithm) learning of how to play snake (the old video game). The state could be a vector of all pixel values (not very efficient to get lots of pixel values as state). The actions could be one of the four directions. The reward could be zero if the snake don't get to an apple and plus one if the snake found a new apple.\n",
    "\n",
    "Another example might be a robot (or algorithm) learning how to drive a car in a specific track. The state could be a vector representing the distance to the walls in eight directions around the car and the velocity of the car. The actions could be one of the following: accelerate, break, move heading direction to right, move heading direction to left. The reward could be normally zero, plus one if the car reaches a checkpoint of the track and minus one if the car touches a wall.\n",
    "\n",
    "A third example might be a reinforcement learning algorithm designed to controll traffic lights in a specific area. The state could be a vector representing the traffic flow for each lane-section in the area. The actions could be predefined phase combinations and the reward function could be defined, such that the reward is related to a reduction in delay compared with previous time steps.\n",
    "\n",
    "b) \n",
    "\n",
    "No matter how long it takes, without a discount the total reward will always be 1. So there is no incentive for the agent to actually improve in any way, as every episode will end with a reward of 1 (as finishing the maze is the only way to terminate). To make the agent work, either add a discount or negative rewards for evert step taken.\n",
    "\n",
    "c)\n",
    "\n",
    "$G_5 = 0$\n",
    "\n",
    "$G_4 = R_5 = 2$\n",
    "\n",
    "$G_3 = 0.5 \\cdot G_4 + R_4 = 4$\n",
    "\n",
    "$G_2 = 0.5 \\cdot G_3 + R_3 = 8$\n",
    "\n",
    "$G_1 = 0.5 \\cdot G_2 + R_2 = 6$\n",
    "\n",
    "$G_0 = 0.5 \\cdot G_1 + R_1 = 2$\n",
    "\n",
    "d) \n",
    "\n",
    "$G_0 = 65$\n",
    "\n",
    "$G_1 = 70$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.99999999999997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([(0.9**k)*7 for k in range(100000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e)\n",
    "\n",
    "$$ v_\\pi (s) = \\sum _a \\pi (a|s)q_\\pi (s,a)$$\n",
    "\n",
    "f)\n",
    "\n",
    "$$ q_\\pi (s,a) = \\sum _{s' ,r} p(s',r|s,a) \\left[r+ \\gamma v_\\pi (s')\\right]$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uni]",
   "language": "python",
   "name": "conda-env-uni-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
