{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<p style=\"text-align:left;\">Reinforcement Learning\n",
    "<span style=\"float:right;\">Monday, 11. May 2020</span></p>\n",
    "\n",
    "<p style=\"text-align:left;\">Prof. S. Harmeling\n",
    "<span style=\"float:right;\">DUE 23:55 Monday, 18. May 2020</span></p>\n",
    "\n",
    "---\n",
    "<p style=\"text-align:center;\"><b>Exercise set #4</b></p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Monte Carlo control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement the **Monte Carlo control** algorithm.  \n",
    "This exercise was inspired by the Reinforcement Learning tutorial by Shimon Whiteson  \n",
    "from the Machine Learning Summer School 2019: https://github.com/mlss-skoltech\n",
    "\n",
    "Monte Carlo (MC) methods are used on episodic tasks where the model is not known (model-free).  \n",
    "The algorithm learns the perfect policy from experience by averaging the sample returns at the end of each episode. \n",
    "\n",
    "In the last exercise we had a perfect model of the FrozenLake environment, where all the transition probabilities between states are defined.  \n",
    "In constrast to this, MC methods learn the probability distributions by generating sample transitions over many episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackjack Environment\n",
    "\n",
    "In this exercise we will work with the **Blackjack** environment from OpenAI's gym library.  \n",
    "\n",
    "### Blackjack rules (according to OpenAI)\n",
    "\n",
    "*Blackjack is a card game where the goal is to obtain cards that sum to as\n",
    " near as possible to 21 without going over.  They're playing against a fixed dealer.*\n",
    "    \n",
    "*Face cards (Jack, Queen, King) have point value 10.\n",
    "Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "This game is placed with an infinite deck (or with replacement).\n",
    "The game starts with each (player and dealer) having one face up and one\n",
    "face down card.*\n",
    "\n",
    "*The player can request additional cards (hit=1) until they decide to stop\n",
    "(stick=0) or exceed 21 (bust).*\n",
    "\n",
    "*After the player sticks, the dealer reveals their facedown card, and draws\n",
    "until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
    "If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "decided by whose sum is closer to 21.  The reward for winning is +1,\n",
    "drawing is 0, and losing is -1.*\n",
    "\n",
    "*The observation of a 3-tuple of: the players current sum,\n",
    "the dealer's one showing card (1-10 where 1 is ace),\n",
    "and whether or not the player holds a usable ace (0 or 1).*\n",
    "\n",
    "This environment corresponds to the version of the blackjack problem\n",
    "described in Example 5.1 in  \n",
    "[Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto.\n",
    "\n",
    "Each state is a 3-tuple of:\n",
    "- the player's current sum $\\in \\{0, 1, \\ldots, 31\\}$,\n",
    "- the dealer's face up card $\\in \\{1, \\ldots, 10\\}$, and\n",
    "- whether or not the player has a usable ace (`no` $=0$, `yes` $=1$).\n",
    "\n",
    "\n",
    "The agent has two potential actions:\n",
    "```\n",
    "    STICK = 0,        \n",
    "    HIT = 1,\n",
    "```\n",
    "Verify this by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the observation space and action space of the environment to see if it matches the description above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "In our setup, policies are functions that take two arguments, ```env``` and ```state```, and return an action based on that state:\n",
    "\n",
    "```\n",
    "def my_policy(env, state):\n",
    "    action = ...\n",
    "    return action\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_simulation_mc(env, policy, num_episodes=1, silent=True, only_last=False):\n",
    "    \"\"\"Simulates episodes of the blackjack environment for a given policy.\n",
    "        \n",
    "        Args:\n",
    "            env (object): An instance of a blackjack environment.\n",
    "            policy (callable): A function that takes an environment and state and returns an action.\n",
    "            num_episodes (int): Number of episodes to return.\n",
    "            silent (bool): If true supress output.\n",
    "            only_last (bool): If true only return last triple for each episode.\n",
    "            \n",
    "        Returns:\n",
    "            episodes (list): List of triples (state, action, reward) encountered during the episodes.\n",
    "        \"\"\"\n",
    "    episodes = []\n",
    "    for i_episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:            \n",
    "            action = policy(env, state)\n",
    "            if not silent:\n",
    "                print(\"state:\", state)\n",
    "                if action == 0:\n",
    "                    print('STICK')\n",
    "                else:\n",
    "                    print('HIT')\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            if not only_last:\n",
    "                episodes.append((state, action, reward))\n",
    "            state = next_state   \n",
    "        \n",
    "        if only_last:\n",
    "            episodes.append((state, action, reward))\n",
    "        \n",
    "        if not silent:\n",
    "            print('End game! Reward:', reward)\n",
    "            if reward > 0:\n",
    "                print('You won :) (lucky you)\\n')  \n",
    "            elif reward == 0:\n",
    "                print('A draw! (lucky you)\\n')  \n",
    "            else:\n",
    "                print('You lost :( (gambling is a bad habit anyway)\\n')\n",
    "    \n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy\n",
    "\n",
    "We use the random policy to test the policy simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env, state):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_simulation_mc(env, random_policy, num_episodes=3, silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic policy\n",
    "\n",
    "Let us investigate a particular policy that may lead us to victory.  \n",
    "What if each time the sum of our cards gets $> 18$, we STICK, and if this sum is $\\leq 18$ we hit and ask for more cards?  \n",
    "\n",
    "Implement a policy that follows this procedure $80\\%$ of the time and $20\\%$ of the time does the opposite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limit_stochastic_policy(env, state):\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_simulation_mc(env, limit_stochastic_policy, num_episodes=3, silent=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo control\n",
    "\n",
    "Since the states constist of tuples, we won't be able to just store the action-values in a 2D array. Therefore we will store them in a ```dict``` instead,  \n",
    "with the states as keys and the corresponding action-values as values, i.e. ```Q[s]``` is a numpy array and s is a tuple.  \n",
    "In the following we will call such a dictionary a *Q-table*.\n",
    "\n",
    "We start with an arbitrary Q-table and compute the corresponding policy. We can improve upon our existing policy by just greedily choosing the  \n",
    "best action at each state as per our knowledge, i.e. the Q-table, and then recomputing the Q-table. Then we repeat this process over and over again.\n",
    "\n",
    "However, we will now face:\n",
    "- Exploration-exploitation problem ($\\epsilon$-greedy policy)\n",
    "- Decreasing an incremental change term (use a constant hyperparameter instead) (i.e. change $1/N$ to some $\\alpha$)\n",
    "\n",
    "Your task is to implement a variant of the following algorithm and to apply it to the blackjack environment:\n",
    "\n",
    "<img src=\"https://github.com/mlss-skoltech/tutorials_week2/raw/master/reinforcement_learning/img/monte_carlo_off.png\" width=\"500\" height=\"473\">\n",
    "\n",
    "First you will need to implement the following function to obtain a policy from a Q-table.  \n",
    "Look at (c) from the algorithm above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_from_Q(Q, epsilon):\n",
    "    \"\"\"Computes an epsilon-greedy policy for a Q-table.\n",
    "    \n",
    "        Args:\n",
    "            Q (dict): The Q-table. It may or may not have an entry for every state.\n",
    "            epsilon (float): The epsilon value.\n",
    "        \n",
    "        Returns:\n",
    "            policy (callable): A function that takes an environment and state and returns an action.\n",
    "    \"\"\"\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    #########################\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the update function for the Q-table. Instead of saving all returns, take a moving average with rate $\\alpha$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(episode, Q, alpha, gamma):\n",
    "    \"\"\"Updates the action-value function estimate using the most recent episode.\n",
    "    \n",
    "        Args:\n",
    "            episode (list): List of triples (state, action, reward) encountered during the episodes.\n",
    "            Q (dict): The current Q-table.\n",
    "            alpha (float): The update factor.\n",
    "            gamma (float): The reward discount factor.\n",
    "        \n",
    "        Returns:\n",
    "            Q (dict): The updated Q-table.\n",
    "    \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    #########################\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test your implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env, num_episodes, generate_episode, alpha, gamma=1.0,\n",
    "               eps_start=1.0, eps_decay=.99999, eps_min=0.05):\n",
    "    \"\"\"Estimates an optimal policy using mc control\"\"\"\n",
    "    \n",
    "    nA = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(nA))  # initialize empty dictionary of arrays\n",
    "    \n",
    "    epsilon = eps_start\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        \n",
    "        if episode % 1000 == 0:\n",
    "            print('\\rEpisode {}/{}'.format(episode, num_episodes), end='', flush=True)\n",
    "        \n",
    "        epsilon = max(epsilon * eps_decay, eps_min)\n",
    "        \n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode(env, policy_from_Q(Q, epsilon))\n",
    "        \n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q(episode, Q, alpha, gamma)\n",
    "        \n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = policy_from_Q(Q, 0)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the algorithm to get the estimated optimal polciy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_policy = mc_control(env, 500000, policy_simulation_mc, alpha=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_simulation_mc(env, mc_policy, num_episodes = 3, silent = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the Monte Carlo policy to the other policies by completing the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wins(policies, num_episodes=10000):\n",
    "    plt.figure()\n",
    "    \n",
    "    for i, (policy, name) in enumerate(policies):\n",
    "        # data is a list containing one tiple (state, action, reward) from the end of each episode\n",
    "        data = policy_simulation_mc(env, policy,  num_episodes=num_episodes, only_last=True)\n",
    "        \n",
    "        # compute the number of wins, draws and losses\n",
    "        #########################\n",
    "        # Write your code here. #\n",
    "        #########################\n",
    "    \n",
    "        plt.bar([1 + ((i - 1) / 4), 2 + ((i - 1) / 4), 3 + ((i - 1) / 4)],\n",
    "                [losses, draws, wins], width=0.2, label=name)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xticks([1, 2, 3], ['losses', 'draws', 'wins'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wins([(random_policy, 'random'), (limit_stochastic_policy, 'limit stochastic'), (mc_policy, 'Monte Carlo')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how your policy behaves in different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_policy(env, policy):\n",
    "\n",
    "    def get_figure(usable_ace, ax):\n",
    "        x_range = np.arange(11, 22)\n",
    "        y_range = np.arange(10, 0, -1)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        Z = np.array([[policy(env, (x,y,usable_ace)) for x in x_range] for y in y_range])\n",
    "        surf = ax.imshow(Z, cmap=plt.get_cmap('Pastel2', 2), vmin=0, vmax=1, extent=[10.5, 21.5, 0.5, 10.5])\n",
    "        plt.xticks(x_range)\n",
    "        plt.yticks(y_range)\n",
    "        plt.gca().invert_yaxis()\n",
    "        ax.set_xlabel('Player\\'s Current Sum')\n",
    "        ax.set_ylabel('Dealer\\'s Showing Card')\n",
    "        ax.grid(color='w', linestyle='-', linewidth=1)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        cbar = plt.colorbar(surf, ticks=[0,1], cax=cax)\n",
    "        cbar.ax.set_yticklabels(['0 (STICK)','1 (HIT)'])\n",
    "            \n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.set_title('Usable Ace')\n",
    "    get_figure(True, ax)\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.set_title('No Usable Ace')\n",
    "    get_figure(False, ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(env, mc_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the true optimal policy $\\pi_*$ that can be found in Figure 5.2 of [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto.  \n",
    "Compare your final estimate to the optimal policy - how close are you able to get? If you are not happy with the performance of your algorithm,  \n",
    "take the time to tweak the decay rate of $\\epsilon$, change the value of $\\alpha$, and/or run the algorithm for more episodes to attain better results.\n",
    "\n",
    "<img src=\"https://github.com/mlss-skoltech/tutorials_week2/raw/master/reinforcement_learning/img/right_policy.png\" width=\"700\" height=\"573\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
