{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<p style=\"text-align:left;\">Reinforcement Learning\n",
    "<span style=\"float:right;\">Monday, 18. May 2020</span></p>\n",
    "\n",
    "<p style=\"text-align:left;\">Prof. S. Harmeling\n",
    "<span style=\"float:right;\">DUE 23:55 Monday, 25. May 2020</span></p>\n",
    "\n",
    "---\n",
    "<p style=\"text-align:center;\"><b>Exercise set #5</b></p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement **Sarsa**, which is a temporal-difference control algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from gym.envs.toy_text.discrete import DiscreteEnv\n",
    "from gym.wrappers.time_limit import TimeLimit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windy Gridworld Environment\n",
    "\n",
    "We will apply Sarsa to the **Windy Gridworld with King's Moves** environment from [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto, p. 130,  \n",
    "which we already implemented below. King's moves just means that we are also allowed to make diagonal moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGridworld(DiscreteEnv):\n",
    "    \n",
    "    UP = 0\n",
    "    RIGHT = 1\n",
    "    DOWN = 2\n",
    "    LEFT = 3\n",
    "    \n",
    "    # king's moves\n",
    "    UP_LEFT = 4\n",
    "    UP_RIGHT = 5\n",
    "    DOWN_LEFT = 6\n",
    "    DOWN_RIGHT = 7\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, kings_moves=True):\n",
    "        self.kings_moves = kings_moves\n",
    "        self.shape = (7, 10)\n",
    "        self.start_state = np.ravel_multi_index((3, 0), self.shape)\n",
    "        self.goal_state = np.ravel_multi_index((3, 7), self.shape)\n",
    "        self.wind = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
    "        \n",
    "        nS = np.prod(self.shape)\n",
    "        nA = 8 if kings_moves else 4\n",
    "        \n",
    "        # transition probabilities\n",
    "        P = {}\n",
    "        for s in range(nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s] = {a: [] for a in range(nA)}\n",
    "            P[s][self.UP] = self._transition_prob(position, (-1, 0))\n",
    "            P[s][self.RIGHT] = self._transition_prob(position, (0, 1))\n",
    "            P[s][self.DOWN] = self._transition_prob(position, (1, 0))\n",
    "            P[s][self.LEFT] = self._transition_prob(position, (0, -1))\n",
    "            if kings_moves:\n",
    "                P[s][self.UP_LEFT] = self._transition_prob(position, (-1, -1))\n",
    "                P[s][self.UP_RIGHT] = self._transition_prob(position, (-1, 1))\n",
    "                P[s][self.DOWN_LEFT] = self._transition_prob(position, (1, -1))\n",
    "                P[s][self.DOWN_RIGHT] = self._transition_prob(position, (1, 1))\n",
    "        \n",
    "        # initial state distribution\n",
    "        isd = np.zeros(nS)\n",
    "        isd[self.start_state] = 1.0\n",
    "        \n",
    "        self._last_state_action = {}\n",
    "        \n",
    "        super().__init__(nS, nA, P, isd)\n",
    "    \n",
    "    def _transition_prob(self, position, move):\n",
    "        y, x = position\n",
    "        wind = self.wind[x]\n",
    "        new_position = (\n",
    "            min(max(y + move[0] - wind, 0), self.shape[0] - 1),\n",
    "            min(max(x + move[1], 0), self.shape[1] - 1)\n",
    "        )\n",
    "        \n",
    "        prob = 1.0\n",
    "        new_state = np.ravel_multi_index(new_position, self.shape)\n",
    "        reward = -1\n",
    "        done = new_state == self.goal_state\n",
    "        return [(prob, new_state, reward, done)]\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        for state in range(self.nS):\n",
    "            if state == self.s:\n",
    "                output = ' x '\n",
    "            elif state == self.start_state:\n",
    "                output = ' S '\n",
    "            elif state == self.goal_state:\n",
    "                output = ' G '\n",
    "            else:\n",
    "                output = ' - '\n",
    "            \n",
    "            y, x = np.unravel_index(state, self.shape)\n",
    "            if x == 0:\n",
    "                output = output.lstrip()\n",
    "            if x == self.shape[1] - 1:\n",
    "                output = output.rstrip() + '\\n'\n",
    "            \n",
    "            print(output, end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an instance of the environment.  \n",
    "In gym we can *wrap* environments with wrapper classes, that can change the behavior of an environment.  \n",
    "We will use this to limit the number of steps per episode to 1000, so the episodes don't get too long, e.g. if a policy does not reach the goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WindyGridworld(kings_moves=True)\n",
    "env = TimeLimit(env, max_episode_steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "In our setup, policies are functions that take two arguments, ```env``` and ```state```, and return an action based on that state:\n",
    "\n",
    "```\n",
    "def my_policy(env, state):\n",
    "    action = ...\n",
    "    return action\n",
    "```\n",
    "\n",
    "Below we implemented a function that runs one rollout of a given policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, policy, render=False):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.\n",
    "    done = False\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        action = policy(env, state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if render:\n",
    "            sleep(0.4)\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another function that runs multiple rollouts of a given policy and averages the total rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, num_rollouts=100):\n",
    "    return sum(rollout(env, policy) for _ in range(num_rollouts)) / num_rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy\n",
    "A random policy selects random actions and ignores the current state.  \n",
    "Let's see how this very simple policy performs on the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def random_policy(env, state):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  -  -  -  -  -  x  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "S  -  -  -  -  -  -  G  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "\n",
      "Total reward: -40.0\n"
     ]
    }
   ],
   "source": [
    "# limit the number of steps, to prevent a very long wait time\n",
    "total_reward = rollout(TimeLimit(env, max_episode_steps=40), random_policy, render=True)\n",
    "print('Total reward:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward: -787.43\n"
     ]
    }
   ],
   "source": [
    "print('Average total reward:', evaluate(env, random_policy, num_rollouts=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the random policy performs poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-greedy policies\n",
    "\n",
    "In the Sarsa algorithm we will compute action-values $Q$, which need to be converted to actual policies.  \n",
    "\n",
    "For example, a *greedy* policy chooses the actions with the highest action-values.  \n",
    "This will be used for the action-values that Sarsa computed after it has *finished*.  \n",
    "```greedy_policy_from_q``` creates a policy function, that greedily chooses actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy_from_q(Q):\n",
    "    def policy(env, state):\n",
    "        return np.argmax(Q[state])\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside Sarsa one typically uses *epsilon-greedy* policies, that choose random actions with probability $\\epsilon$,  \n",
    "and choose actions with the highest action-value with probability $1 - \\epsilon$.  \n",
    "\n",
    "This can lead to better results, because choosing random actions from time to time increases the *exploration* of the state space.\n",
    "\n",
    "Implement the following function, that converts action-values to an epsilon-greedy policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy_from_q(Q, epsilon):\n",
    "    def policy(env, state):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(range(len(Q[state])))\n",
    "        else:\n",
    "            return np.argmax(Q[state])\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    #########################\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa\n",
    "\n",
    "Now we finally implement Sarsa, that tries to find the optimal action-value function $Q(s,a)$.\n",
    "\n",
    "Implement the Sarsa algorithm from [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto, p. 130:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, epsilon, alpha, gamma, num_episodes):\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    for e in range(num_episodes):\n",
    "        S = env.reset()\n",
    "        done = False\n",
    "        while True:\n",
    "            policy = epsilon_greedy_policy_from_q(Q, epsilon)\n",
    "            A = int(policy(env, S))\n",
    "            assert A in range(env.nA), f'{A}, {env.nA}'\n",
    "            S_, R, done, info = env.step(A)\n",
    "            if done:\n",
    "                break\n",
    "            A_ = int(policy(env, S_))\n",
    "            Q[S, A] += alpha*(R + gamma*(Q[S_, A_]) - Q[S, A])\n",
    "            S = S_\n",
    "    #########################\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should run sarsa multiple times to have a greater chance of finding the optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sarsa(env, epsilon, alpha, gamma, num_episodes, num_runs):\n",
    "    best_avg_total_reward = None\n",
    "    best_Q = None\n",
    "    best_policy = None\n",
    "    \n",
    "    # run num_runs times and store the policy which has the highest average total reward\n",
    "    for _ in range(num_runs):\n",
    "        Q = sarsa(env, epsilon, alpha, gamma, num_episodes)\n",
    "        policy = greedy_policy_from_q(Q)\n",
    "        \n",
    "        avg_total_reward = evaluate(env, policy, num_rollouts=10)\n",
    "        if best_avg_total_reward is None or avg_total_reward > best_avg_total_reward:\n",
    "            best_avg_total_reward = avg_total_reward\n",
    "            best_Q = Q\n",
    "            best_policy = policy\n",
    "    \n",
    "    return best_Q, best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, policy = run_sarsa(env, epsilon=0.1, alpha=0.5, gamma=1, num_episodes=200, num_runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "S  -  -  -  -  -  -  x  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "\n",
      "Total reward: -7.0\n"
     ]
    }
   ],
   "source": [
    "total_reward = rollout(TimeLimit(env, max_episode_steps=40), policy, render=True)\n",
    "print('Total reward:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward: -7.0\n"
     ]
    }
   ],
   "source": [
    "print('Average total reward:', evaluate(env, policy, num_rollouts=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the path\n",
    "Let's visualize the path that the policy takes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_path(env, policy):\n",
    "    action_symbols = ['↑', '→', '↓', '←', '↖', '↗', '↙', '↘']\n",
    "    \n",
    "    state_actions = {}\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        action = policy(env, state)\n",
    "        state_actions[state] = action\n",
    "        state, reward, done, info = env.step(action)\n",
    "    terminal_state = state\n",
    "    \n",
    "    for state in range(env.nS):\n",
    "        if state == terminal_state:\n",
    "            output = ' x '\n",
    "        elif state in state_actions:\n",
    "            action = state_actions[state]\n",
    "            output = ' ' + action_symbols[action] + ' '\n",
    "        else:\n",
    "            output = ' - '\n",
    "        \n",
    "        y, x = np.unravel_index(state, env.shape)\n",
    "        if x == 0:\n",
    "            output = output.lstrip()\n",
    "        if x == env.shape[1] - 1:\n",
    "            output = output.rstrip() + '\\n'\n",
    "        \n",
    "        print(output, end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "↘  -  -  -  -  -  -  x  -  -\n",
      "-  ↘  -  -  -  -  -  -  -  -\n",
      "-  -  ↘  -  -  -  →  -  -  -\n",
      "-  -  -  ↘  ↘  →  -  -  -  -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_path(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without King's moves\n",
    "Let's see what happens, if we disable diagonal moves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_straight = TimeLimit(WindyGridworld(kings_moves=False), max_episode_steps=1000)\n",
    "Q_straight, policy_straight = run_sarsa(env_straight, epsilon=0.1, alpha=0.5, gamma=1, num_episodes=200, num_runs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "S  -  -  -  -  -  -  x  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "\n",
      "Total reward: -15.0\n"
     ]
    }
   ],
   "source": [
    "total_reward = rollout(env_straight, policy_straight, render=True)\n",
    "print('Total reward:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward: -15.0\n"
     ]
    }
   ],
   "source": [
    "print('Average total reward:', evaluate(env_straight, policy_straight, num_rollouts=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  -  -  -  -  -  →  →  →  ↓\n",
      "-  -  -  -  -  →  -  -  -  ↓\n",
      "-  -  -  -  →  -  -  -  -  ↓\n",
      "→  →  →  →  -  -  -  x  -  ↓\n",
      "-  -  -  -  -  -  -  -  ←  ←\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "-  -  -  -  -  -  -  -  -  -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualize_path(env_straight, policy_straight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uni]",
   "language": "python",
   "name": "conda-env-uni-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
