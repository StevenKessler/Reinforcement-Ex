{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<p style=\"text-align:left;\">Reinforcement Learning\n",
    "<span style=\"float:right;\">Monday, 08. June 2020</span></p>\n",
    "\n",
    "<p style=\"text-align:left;\">Prof. S. Harmeling\n",
    "<span style=\"float:right;\">DUE 23:55 Monday, 15. June 2020</span></p>\n",
    "\n",
    "---\n",
    "<p style=\"text-align:center;\"><b>Exercise set #8</b></p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sarsa(λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement **Sarsa(λ)** with replacing traces and $\\epsilon$-greedy action selection.  \n",
    "(Example 10.1 from [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto, p. 244ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import tiles3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (8, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain Car Environment\n",
    "\n",
    "In this exercise we will work with the **Mountain Car** environment from OpenAI's gym library.  \n",
    "The three actions (decelerate, coast, and accelerate) in the mountain car problem are represented by the integers 0, 1, and 2.  \n",
    "The states are represented by a numpy array of doubles corresponding to the position and velocity of the car.  \n",
    "Make yourself familiar with the environment:\n",
    "- https://gym.openai.com/envs/MountainCar-v0\n",
    "- https://github.com/openai/gym/wiki/MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0').env\n",
    "env = gym.wrappers.TimeLimit(env, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random policy reached a total reward of -1000.0\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print('The random policy reached a total reward of', total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sarsa(λ)\n",
    "\n",
    "We will implement the Sarsa(λ) algorithm from Chapter 12.7 from [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto, p. 303ff.  \n",
    "We are going to use **replacing traces** and **ϵ-greedy action selection**.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/3516/1*wcb9rZn27woQaliD7h4DLQ.png\" width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tile coding\n",
    "\n",
    "In this assignment we will use **tile coding** to convert the two-dimensional state vector into\n",
    "a binary state-action feature vector  \n",
    "(for more details see Chapter 9.5.4 from [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto, p.217ff).  \n",
    "\n",
    "We have provided the tile coding software by Richard Sutton in ```tiles3.py```.  \n",
    "To understand how to use it you should take a look inside this file.  \n",
    "\n",
    "To do tile coding properly you have to: \n",
    "- account for the shape of the tilings and ranges of the position and velocity before calling the tile coder, and \n",
    "- incorporate the action into the call to the tile coder.\n",
    "\n",
    "We will use the following parameter settings:  \n",
    "- memorySize = 4096 (for the tile coder)\n",
    "- num tilings = 8; shape/size of tilings = 8x8\n",
    "- $\\alpha = 0.1 /$(num tilings)\n",
    "- $\\lambda = 0.9$\n",
    "- $\\epsilon = 0.0$\n",
    "- initial weights = random numbers between 0 and -0.001\n",
    "- $\\gamma = 1$\n",
    "\n",
    "Implement the missing parts of the Sarsa agent below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.s_min = env.observation_space.low\n",
    "        self.s_max = env.observation_space.high\n",
    "        self.iht = tiles3.IHT(4096)\n",
    "        self.w = np.random.uniform(-0.001, 0., 4096)\n",
    "    \n",
    "    def action_value(self, s, a):\n",
    "        #########################\n",
    "        # Write your code here. #\n",
    "        q = np.sum(self.w[self.features(s,a)])\n",
    "        #########################\n",
    "        return q\n",
    "    \n",
    "    def features(self, s, a):\n",
    "        #########################\n",
    "        scale_pos = 8 / (self.s_max[0] - self.s_min[0])\n",
    "        scale_velo = 8 / (self.s_max[1] - self.s_min[1])\n",
    "        x = tiles3.tiles(self.iht, 8, (s[0]*scale_pos, s[1]*scale_velo), [a])\n",
    "        #########################\n",
    "        return x\n",
    "    \n",
    "    def choose_action(self, env, s, epsilon=0):\n",
    "        #########################\n",
    "        if np.random.binomial(1, epsilon) == 1:\n",
    "            action = np.random.randint(0, 3)\n",
    "        else:\n",
    "            action = np.argmax([self.action_value(s, a) for a in range(0,3)])\n",
    "        return action\n",
    "        #########################\n",
    "    \n",
    "    def train(self, env, alpha, lamda, gamma, epsilon):\n",
    "        state = env.reset()\n",
    "        action = self.choose_action(env, state, epsilon)\n",
    "        z = np.zeros(4096)\n",
    "        num_steps = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            #########################\n",
    "            delta = reward\n",
    "            F = self.features(state, action)\n",
    "            for i in range(len(F)):\n",
    "                idx = F[i]\n",
    "                delta -= self.w[idx]\n",
    "                z[idx] = 1\n",
    "            if done:\n",
    "                self.w = self.w + alpha * delta * z\n",
    "            action = self.choose_action(env, next_state, epsilon)\n",
    "            next_next_state, reward, done, info = env.step(action)\n",
    "            F = self.features(next_state, action)\n",
    "            for i in range(len(F)):\n",
    "                idx = F[i]\n",
    "                delta += gamma * self.w[idx]\n",
    "            self.w = self.w + alpha * delta * z\n",
    "            z = gamma * lamda * z\n",
    "            state = next_state\n",
    "\n",
    "            #########################\n",
    "            \n",
    "            num_steps += 1\n",
    "            #if num_steps % 100 == 0:\n",
    "                #print(f'pos{state[0]}')\n",
    "        return num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Now we can test your implementation. We produce a learning curve for the Sarsa agent and plot the number of steps  \n",
    "per episode (x-axis), over 200 episodes (y-axis), averaged over 10 independent runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 6/10"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "num_episodes = 200\n",
    "\n",
    "avg_num_steps = np.zeros(num_episodes)\n",
    "\n",
    "for run in range(1, num_runs + 1):\n",
    "    print('\\rRun: {}/{}'.format(run, num_runs), end='', flush=True)\n",
    "    agent = SarsaAgent(env)\n",
    "    for episode in range(num_episodes):\n",
    "        num_steps = agent.train(env, alpha=0.1/8, lamda=0.9, gamma=1, epsilon=0)\n",
    "        avg_num_steps[episode] += num_steps\n",
    "\n",
    "avg_num_steps /= num_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps per episode')\n",
    "plt.yscale('log')\n",
    "plt.yticks([100, 200, 400, 1000])\n",
    "plt.gca().get_yaxis().set_major_formatter(mpl.ticker.ScalarFormatter())\n",
    "plt.plot(avg_num_steps);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:uni]",
   "language": "python",
   "name": "conda-env-uni-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
