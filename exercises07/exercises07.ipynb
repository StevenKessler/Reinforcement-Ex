{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<p style=\"text-align:left;\">Reinforcement Learning\n",
    "<span style=\"float:right;\">Monday, 08. June 2020</span></p>\n",
    "\n",
    "<p style=\"text-align:left;\">Prof. S. Harmeling\n",
    "<span style=\"float:right;\">DUE 23:55 Monday, 15. June 2020</span></p>\n",
    "\n",
    "---\n",
    "<p style=\"text-align:center;\"><b>Exercise set #7</b></p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Semi-gradient TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will implement **semi-gradient TD(0)**, which is an approximate temporal-difference prediction algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (8, 5)\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['grid.linestyle'] = 'dashed'\n",
    "mpl.rcParams['grid.color'] = 'black'\n",
    "mpl.rcParams['grid.alpha'] = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1000-state Random Walk Environment\n",
    "\n",
    "We will apply semi-gradient TD(0) to the **1000-state Random Walk** environment from  \n",
    "[Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) by Sutton and Barto, p. 203.  \n",
    "\n",
    "We implemented some parts of the environment in the code below. To fit in the gym library,  \n",
    "the states are numbered from 0 to 999 (instead of 1 to 1000) and the start state is 499.  \n",
    "\n",
    "Implement the ```step()``` method of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalk1000(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = gym.spaces.Discrete(1000)\n",
    "        self.action_space = gym.spaces.Discrete(0)\n",
    "        self.state = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 499\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action=None):\n",
    "        #########################\n",
    "        # Write your code here. #\n",
    "        #########################\n",
    "        info = {}\n",
    "        return self.state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create an instance of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RandomWalk1000()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-gradient TD(0)\n",
    "\n",
    "### Value functions\n",
    "\n",
    "Remember that TD(0) is a *prediction* method, i.e. we use it to *approximate the value function* $v_\\pi$ of a given policy $\\pi$.  \n",
    "\n",
    "In the previous exercises we studied *tabular* methods, where we could store the state-values in an array ```V```,  \n",
    "since the state space was relatively small. For larger (or continuous) state spaces we may not be able to store  \n",
    "all values explicitly, but instead we approximate the value function with *parameterized functions*:\n",
    "\n",
    "$$\\hat{v}_w(s) \\approx v_\\pi(s)$$\n",
    "\n",
    "where $\\hat{v}_w(s)$ is a parameterized function with weight vector $w \\in \\mathbb{R}^d$.  \n",
    "\n",
    "In real-world applications $\\hat{v}_w(s)$ may be a neural network, but we will take a look at the most basic class of functions,  \n",
    "i.e. *linear functions*:\n",
    "$$\\hat{v}_w(s) = w^Tx(s)$$\n",
    "\n",
    "Here, $x(s)$ is called a *feature vector* that represents state $s$. It has the same number of components as $w$.  \n",
    "We need the feature vector, so that we have some internal representation of the state and can take an  \n",
    "inner product with the weights.\n",
    "\n",
    "Now, to find a good approximation of the value function, we need to find suitable weights $w$, by starting with  \n",
    "some arbitrary weight initialization and iteratively updating the weights with stochastic gradient descent (SGD).  \n",
    "This is exactly what semi-gradient TD(0) does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 1000-state Random Walk environment we will take a look at two ways to compute feature vectors:  \n",
    "- **One-hot encoding**: A unique feature vector for each state, where the number of components  \n",
    "  in the feature vector is equal to the number of states. The components are all zero, except the  \n",
    "  component of the state, which is one (e.g. the feature vector of the third state in an environment  \n",
    "  with four states would be $[0, 0, 1, 0]^T$).\n",
    "- **State aggregation**: Similar to the one-hot encoding, but states are partitioned into groups.  \n",
    "  The number of components in the feature vector is equal to the number of groups.  \n",
    "  The group that a state belongs to gets set to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we partially implemented linear value functions for the 1000-state Random Walk environment:\n",
    "- ```LinearValueFunction``` is an abstract class that stores a weight vector ```w``` and computes state-values with ```value(s)```.  \n",
    "  The weights can be updated with the ```train(env, alpha, gamma)``` method, which uses semi-gradient TD(0).\n",
    "\n",
    "\n",
    "- ```OneHotValueFunction``` is a subclass of ```LinearValueFunction``` and computes one-hot encoded features.  \n",
    "  For the 1000-state Random Walk environment, x(s) has 1000 components.\n",
    "\n",
    "\n",
    "- ```StateAggregationValueFunction``` is a subclass of ```LinearValueFunction``` and performs state aggregation.  \n",
    "  For the 1000-state Random Walk environment, there should be 10 groups of size 100.\n",
    "\n",
    "Implement the missing parts in the classes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearValueFunction(ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # w are the weights of this linear function, which get initialized in the subclasses\n",
    "        self.w = None\n",
    "    \n",
    "    def values(self):\n",
    "        \"\"\"Returns an array of the approximated values of all states.\"\"\"\n",
    "        return np.array([self.value(s) for s in range(1000)])\n",
    "    \n",
    "    def value(self, state):\n",
    "        \"\"\"Returns the approximated value of a state.\"\"\"\n",
    "        #########################\n",
    "        # Write your code here. #\n",
    "        #########################\n",
    "    \n",
    "    def value_grad(self, state):\n",
    "        \"\"\"Returns the gradient of the approximated value of a state.\"\"\"\n",
    "        #########################\n",
    "        # Write your code here. #\n",
    "        #########################\n",
    "    \n",
    "    @abstractmethod\n",
    "    def features(self, state):\n",
    "        \"\"\"Returns the feature vector x(state), which used to compute the value.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def train(self, env, alpha, gamma):\n",
    "        \"\"\"Runs one episode in the environment and updates the weights via semi-gradient TD(0).\n",
    "        - env: The environment.\n",
    "        - alpha: The step size.\n",
    "        - gamma: The discount-rate.\n",
    "        \"\"\"\n",
    "        #########################\n",
    "        # Write your code here. #\n",
    "        #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotValueFunction(LinearValueFunction):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.w = np.zeros(1000)\n",
    "    \n",
    "    def features(self, state):\n",
    "        \"\"\"Computes one hot-encoded features.\"\"\"\n",
    "        #########################\n",
    "        # Write your code here. #\n",
    "        #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateAggregationValueFunction(LinearValueFunction):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # initialize the weights\n",
    "        self.w = np.zeros(10)\n",
    "    \n",
    "    def features(self, state):\n",
    "        \"\"\"Performs state aggregation. For the 1000-state Random Walk environment,\n",
    "        there should be 10 groups of size 100.\"\"\"\n",
    "        #########################\n",
    "        # Write your code here. #\n",
    "        #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS error\n",
    "\n",
    "To visualize how the prediction improves over time we will compute the root-mean-square (RMS) error  \n",
    "to measure the difference between the predicted values and the true values. In a real world application  \n",
    "this would not be possible, since we would not know the true values in the first place.  \n",
    "\n",
    "We provide you the true values in the file ```true_values.npy```, which we will load in a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = np.load('true_values.npy')\n",
    "\n",
    "plt.title('True value')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(np.arange(1001, step=100))\n",
    "plt.grid(True)\n",
    "plt.plot(true_values, c='#601A4a');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the RMS error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_error(values, true_values):\n",
    "    \"\"\"Computes the root-mean-squared error.\n",
    "    - values: The predicted values.\n",
    "    - true_values: The true values.\n",
    "    \"\"\"\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Now we will actually predict the values. The function below runs the semi-gradient TD(0) algorithm  \n",
    "multiple times and computes the RMS error after each episode. The values and the RMS errors are  \n",
    "averaged over the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(env, true_values, value_cls, alpha, gamma, num_episodes, num_runs):\n",
    "    avg_values = np.zeros(1000)\n",
    "    avg_rms_errors = np.zeros(num_episodes)\n",
    "    \n",
    "    for run in range(1, num_runs + 1):\n",
    "        print('\\rRun: {}/{}'.format(run, num_runs), end='', flush=True)\n",
    "        \n",
    "        value_fn = value_cls()\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            value_fn.train(env, alpha, gamma)\n",
    "            avg_rms_errors[episode] += rms_error(value_fn.values(), true_values)\n",
    "        \n",
    "        avg_values += value_fn.values()\n",
    "    \n",
    "    avg_values /= num_runs\n",
    "    avg_rms_errors /= num_runs\n",
    "    return avg_values, avg_rms_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will predict the values for both types of feature vectors.  \n",
    "The random seeds should be the same to make the results comparable.  \n",
    "\n",
    "First we will use the **one-hot encoded features**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "avg_values_one_hot, avg_rms_errors_one_hot = \\\n",
    "    run_prediction(env, true_values, OneHotValueFunction, alpha=0.5, gamma=1, num_episodes=3000, num_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use **state aggregation**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "avg_values_state_aggr, avg_rms_errors_state_aggr = \\\n",
    "    run_prediction(env, true_values, StateAggregationValueFunction, alpha=0.1, gamma=1, num_episodes=3000, num_runs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "After successfully running the prediction algorithm, we visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Semi-gradient TD(0) results')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Value')\n",
    "plt.plot(avg_values_one_hot, c='#63ACBE', label='One-hot')\n",
    "plt.plot(avg_values_state_aggr, c='#EE442F', label='State aggregation')\n",
    "plt.plot(true_values, c='#601A4a', label='True values')\n",
    "plt.xticks(np.arange(1001, step=100))\n",
    "plt.grid(True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Episode')\n",
    "plt.ylabel('RMS error')\n",
    "plt.plot(avg_rms_errors_one_hot, c='#63ACBE', label='One-hot')\n",
    "plt.plot(avg_rms_errors_state_aggr, c='#EE442F', label='State aggregation')\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
