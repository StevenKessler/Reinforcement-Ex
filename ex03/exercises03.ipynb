{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<p style=\"text-align:left;\">Reinforcement Learning\n",
    "<span style=\"float:right;\">Monday, 04. May 2020</span></p>\n",
    "\n",
    "<p style=\"text-align:left;\">Prof. S. Harmeling\n",
    "<span style=\"float:right;\">DUE 23:55 Monday, 11. May 2020</span></p>\n",
    "\n",
    "---\n",
    "<p style=\"text-align:center;\"><b>Exercise set #3</b></p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Policy iteration\n",
    "\n",
    "In this exercise you will implement **policy iteration**, which is a dynamic programming algorithm.  \n",
    "This exercise was inspired by the Reinforcement Learning tutorial by Shimon Whiteson  \n",
    "from the Machine Learning Summer School 2019: https://github.com/mlss-skoltech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake Environment\n",
    "\n",
    "In this exercise we will work with the Frozen Lake environment from OpenAI's gym library.  \n",
    "Make yourself familiar with the environment:\n",
    "- https://gym.openai.com/envs/FrozenLake-v0\n",
    "- https://github.com/openai/gym/wiki/FrozenLake-v0\n",
    "\n",
    "The environment also provides some useful attributes:\n",
    "- ```env.nS```: the number of states\n",
    "- ```env.nA```: the number of actions\n",
    "- ```env.P```: contains the transition probabilities of state-action pairs, i.e.\n",
    "    ```\n",
    "    prob, next_state, reward, done = env.P[state][action]\n",
    "    ```\n",
    "    where ```prob``` is the probability $p(s',r|s,a)$, that ```state``` and ```action``` lead to ```next_state``` and ```reward```\n",
    "\n",
    "Now, let's create an instance of the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0').env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "In our setup, policies are functions that take two arguments, ```env``` and ```state```, and return an action based on that state:\n",
    "\n",
    "```\n",
    "def my_policy(env, state):\n",
    "    action = ...\n",
    "    return action\n",
    "```\n",
    "\n",
    "Below we implemented a function that runs one rollout of a given policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(env, policy, render=False):\n",
    "    state = env.reset()\n",
    "    total_reward = 0.\n",
    "    done = False\n",
    "    while not done:\n",
    "        if render:\n",
    "            env.render()\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "        action = policy(env, state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        if render:\n",
    "            sleep(0.4)\n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another function that runs multiple rollouts of a given policy and averages the total rewards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, num_rollouts=100):\n",
    "    return sum(rollout(env, policy) for _ in range(num_rollouts)) / num_rollouts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random policy\n",
    "The random policy selects random actions and ignores the current state.  \n",
    "Let's see how this very simple policy performs on the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env, state):\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = rollout(env, random_policy, render=True)\n",
    "print('\\nTotal reward:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average total reward:', evaluate(env, random_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the random policy performs poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-deterministic policies\n",
    "We will use non-deterministic policies, that define conditional probabilities over the actions given a state, i.e. $\\pi(a|s)$.  \n",
    "Since we work with finite state and action spaces, we can store these conditional probabilities in a 2D array ```pi``` of shape ```(env.nS, env.nA)```,  \n",
    "such that ```pi[state, action]``` corresponds to $\\pi(a|s)$.\n",
    "\n",
    "```policy_from_pi``` creates a policy function, that randomly chooses an action based on the conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_from_pi(pi):\n",
    "    def policy(env, state):\n",
    "        action_probs = pi[state]\n",
    "        return np.random.choice(np.arange(env.nA), p=action_probs)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration\n",
    "We will follow the policy iteration algorithm from *Reinforcement Learning: An Introduction* by Sutton and Barto, p. 80  \n",
    "(http://incompleteideas.net/book/the-book-2nd.html), but because we use non-deterministic policies, the implementation will be slightly different.\n",
    "\n",
    "### Policy evaluation\n",
    "\n",
    "We want to determine the value function $V_\\pi(s)$ for a given policy $\\pi$.  \n",
    "Since we work with finite state spaces, we can store the values for each state in an array ```V```,  \n",
    "such that ```V[s]``` corresponds to $V(s)$.\n",
    "\n",
    "```V``` is initialized with zeros, then we iteratively apply the *Bellman expectation equation*, until the values converge.  \n",
    "Because we use non-deterministic policies, the equation looks slightly different from the one in the book:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "V_\\pi(s) & = \\mathbb{E} [r + \\gamma V_\\pi(s') \\,|\\, s] \\\\[3pt]\n",
    "         & = \\sum_a \\pi(a|s) \\sum_{s',r} \\ p(s',r|s,a) \\ [r + \\gamma V_\\pi(s')] \\\\\n",
    "         & = \\sum_a \\pi(a|s)\\ Q_\\pi(s,a)\n",
    "\\end{aligned}$$\n",
    "\n",
    "In the last step we used the fact that the action-value function can be expressed in terms of the state-value function:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Q_\\pi(s,a) & = \\mathbb{E} [r + \\gamma V_\\pi(s') \\,|\\, s, a] \\\\[3pt]\n",
    "           & = \\sum_{s',r} p(s',r|s,a)\\ [r + \\gamma V_\\pi(s')]\n",
    "\\end{aligned}$$\n",
    "\n",
    "Implement this last equation, since it will be very handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_value(env, V, state, action, gamma):\n",
    "    \"\"\"Computes the action-value Q(s,a) for a given state-action pair (state, action)\n",
    "    based on the state-value function.\n",
    "    - gamma: The discount-rate.\n",
    "    \"\"\"\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    #########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the ```action_value()``` function and implement policy evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, pi, gamma, theta):\n",
    "    \"\"\"Computes the state-value function V of a policy pi.\n",
    "    - gamma: The discount-rate.\n",
    "    - theta: A small threshold, determining when the values converge (see algorithm p. 80).\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.nS)\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    #########################\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy improvement\n",
    "\n",
    "Now we want to find the best policy for the value function that we computed in policy evaluation,  \n",
    "by maximizing the the action-value function.  \n",
    "\n",
    "Again, because we use non-deterministic policies, the implementation will slightly differ from the book.  \n",
    "Instead of just using $\\arg\\max$, we assign probabilities. If there are multiple maximizing actions,  \n",
    "we evenly distribute their probabilities:\n",
    "$$\\pi'(a|s) := \\begin{cases}\n",
    "    \\frac{1}{|\\arg\\max_a Q_\\pi(s,a)\\,|} & \\text{if } a \\in \\arg\\max_a Q_\\pi(s,a) \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Furthermore, to determine if the policy is *stable* we need to check if the *state-values* no longer change,  \n",
    "instead of checking if the actions change.\n",
    "\n",
    "Therefore only implement the loop of policy improvement from the book and do *not* check if the policy is stable here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma):\n",
    "    pi = np.zeros((env.nS, env.nA))\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    #########################\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration\n",
    "\n",
    "Finally, we arrive at policy iteration by iteratively performing policy evaluation and policy improvement,  \n",
    "until the policy is stable. Stop if the policy is stable, i.e. if the state-values no longer change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma, theta):\n",
    "    pi = np.full((env.nS, env.nA), 1 / env.nA)  # initialize with random policy\n",
    "    #########################\n",
    "    # Write your code here. #\n",
    "    #########################\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = policy_iteration(env, gamma=1, theta=1e-8)\n",
    "policy = policy_from_pi(pi)\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = rollout(env, policy, render=True)\n",
    "print('\\nTotal reward:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average total reward:', evaluate(env, policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should achieve an average total reward of at least 0.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
